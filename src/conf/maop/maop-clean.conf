[train]
seed: 72000
model_type: MultimodalASRFeats
patience: 10
max_epochs: 100
eval_freq: 1000
eval_metrics: wer,loss
# this is a bpe model so de-bpe is necessary for correct WER computation
eval_filters: de-bpe
eval_beam: 5
eval_batch_size: 16
save_best_metrics: True
eval_max_len: 400
n_checkpoints: 0
l2_reg: 0
gclip: 1
optimizer: adam
lr: 0.0004
lr_decay: plateau
lr_decay_revert: False
lr_decay_factor: 0.5
lr_decay_patience: 2
batch_size: 36
save_path: ../exp/asr-clean
data_path: /project/ocean/tsriniva/data_MultimodalASR                               # REPLACE WITH <project_dir>/data
pretrained_file: ${data_path}/models/speechcoco_pretrain_ckpts/maop.ckpt 
tensorboard_dir: ${save_path}/tb


[model]
att_type: mlp
att_bottleneck: hid
feat_dim: 43
enc_dim: 256
proj_dim: 256
emb_dim: 256
dec_dim: 256
dropout: 0.4
# 6 encoder layers
enc_layers: '1_1_2_2_1_1'
tied_dec_embs: True
dec_init: mean_ctx
bucket_by: en_speech
# Enough coverage @ 1500
max_len: 1500
feat_fusion: bbox_attn
aux_dim: 2048
aux_proj_dim: 256
aux_activ: tanh
feat_activ: tanh

direction: en_speech:Kaldi, feats:NumpySequence -> en_text:Text

[data]
root: ${train:data_path}/fbank_feats/data
text_root: ${train:data_path}/text_files
feats_root: ${train:data_path}/visual_feats/proposal_feats
 
train_set: {'en_speech': '${root}/train_clean',
            'feats': '${feats_root}/train_bboxes.txt',
            'en_text': '${text_root}/train.en'}

val_set: {'en_speech': '${root}/dev_clean',
          'feats': '${feats_root}/dev_bboxes.txt',
          'en_text': '${text_root}/dev.en'}

test_set: {'en_speech': '${root}/test_clean',
           'feats': '${feats_root}/test_bboxes.txt',
           'en_text': '${text_root}/test.en'}


[vocabulary]
en_text: ${data:text_root}/flickr8k-min10.vocab


